# Python Test Case Generation using GPT

This project uses the plum API to take a pre-selected list of methods from a pre-selected list of repos as prompts, generates test cases using the GPT (GPT-3.5 or GPT-4) model for each prompt repo, 
executes the generated test cases, and provides a summary of the results. This script uses the focal function prompts defined in `baseline.hahses`.
This is a useful tool for developers working with Python to validate their code and generate test cases based on their functions.

The project consists of four Python scripts:
1. collect.py
2. generate.py
3. execute.py
4. results.py

## Requirements & Setup

### Python dependencies

- Python 3.x
- pandas
- tqdm
- openai
- ray
- pathlib
- argparse
- pytest (for running tests)

Install the dependencies using the following command:

```sh
pip install pandas tqdm openai ray pathlib argparse pytest
```

### Directory structure and Code Specifications

The project expects the following directories and files:

- experiments: This folder will store the output JSON files from each of the scripts.
- repos: This folder will store the repositories from which the functions and test cases will be extracted.

The inputs that the user needs to define are as follows:
- experiment_name: this will become \{experiment_name\}.jsonl, where the information from running each of the steps of the project is stored
- base: path to the git cloned repos whose methods you will be generating tests for
- repo-list: path to list of repos 
- hashes: path to list of hashes for the selected methods (Note: for now, use baseline.hahses. The code to create your own hash list will come in a future merge)


## Running the project

Follow the step-by-step instructions below to run the project:

1. Collect test case prompts using `collect.py`. This script extracts function information from the given repositories and prepares prompts for test case generation.

```sh
python collect.py --base /path/to/repos --repo-list /path/to/repo_list.csv --hashes baseline.hahses -e experiment_name
```

2. Generate test cases using the GPT model with `generate.py`. This script sends the prompts prepared in the previous step to the GPT model and retrieves the generated test cases.

```sh
python generate.py --experiment-name experiment_name --model-name gpt3.5
```

3. Execute the generated test cases with `execute.py`. This script runs the test cases and records the results.

```sh
python execute.py --experiment-name experiment_name --base /path/to/repos --model-name gpt3.5
```

4. Analyze results using the `results.py` script. This script provides a summary of the overall results.

```sh
python results.py --experiment-name experiment_name --model-name gpt3.5
```

## Output

After running all scripts, you will find the following JSON files in the `experiments` folder:

- experiment_name.jsonl: Contains the test case prompts.
- experiment_name-completions-gpt3.5.jsonl: Contains the test case completions generated by GPT.
- experiment_name-execution-gpt3.5.jsonl: Contains the test case execution results.
- results.jsonl: Contains the summary results.

The results.jsonl file will contain information like the number of repositories processed, total methods, passing tests, pass@1, syntax errors, import errors, failing tests, and build errors.

